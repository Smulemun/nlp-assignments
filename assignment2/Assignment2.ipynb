{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIgM6C9HYUhm"
      },
      "source": [
        "# Context-sensitive Spelling Correction\n",
        "\n",
        "The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n",
        "\n",
        "Submit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n",
        "\n",
        "Useful links:\n",
        "- [Norvig's solution](https://norvig.com/spell-correct.html)\n",
        "- [Norvig's dataset](https://norvig.com/big.txt)\n",
        "- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n",
        "\n",
        "Grading:\n",
        "- 60 points - Implement spelling correction\n",
        "- 20 points - Justify your decisions\n",
        "- 20 points - Evaluate on a test set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-vb8yFOGRDF"
      },
      "source": [
        "## Implement context-sensitive spelling correction\n",
        "\n",
        "Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n",
        "\n",
        "The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n",
        "\n",
        "You may also want to implement:\n",
        "- spell-checking for a concrete language - Russian, Tatar, etc. - any one you know, such that the solution accounts for language specifics,\n",
        "- some recent (or not very recent) paper on this topic,\n",
        "- solution which takes into account keyboard layout and associated misspellings,\n",
        "- efficiency improvement to make the solution faster,\n",
        "- any other idea of yours to improve the Norvigâ€™s solution.\n",
        "\n",
        "IMPORTANT:  \n",
        "Your project should not be a mere code copy-paste from somewhere. You must provide:\n",
        "- Your implementation\n",
        "- Analysis of why the implemented approach is suggested\n",
        "- Improvements of the original approach that you have chosen to implement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MoQeEsZvHvvi"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>freq</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>16</td>\n",
              "      <td>a</td>\n",
              "      <td>babe</td>\n",
              "      <td>in</td>\n",
              "      <td>the</td>\n",
              "      <td>woods</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6</td>\n",
              "      <td>a</td>\n",
              "      <td>baby</td>\n",
              "      <td>at</td>\n",
              "      <td>her</td>\n",
              "      <td>breast</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9</td>\n",
              "      <td>a</td>\n",
              "      <td>baby</td>\n",
              "      <td>brother</td>\n",
              "      <td>or</td>\n",
              "      <td>sister</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>a</td>\n",
              "      <td>baby</td>\n",
              "      <td>crying</td>\n",
              "      <td>in</td>\n",
              "      <td>the</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>a</td>\n",
              "      <td>baby</td>\n",
              "      <td>girl</td>\n",
              "      <td>was</td>\n",
              "      <td>born</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   freq  1     2        3    4       5\n",
              "0    16  a  babe       in  the   woods\n",
              "1     6  a  baby       at  her  breast\n",
              "2     9  a  baby  brother   or  sister\n",
              "3     6  a  baby   crying   in     the\n",
              "4     6  a  baby     girl  was    born"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "from tqdm.notebook import tqdm\n",
        "from collections import Counter, defaultdict\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import random\n",
        "\n",
        "punct = string.punctuation + '... '\n",
        "\n",
        "N = 5\n",
        "\n",
        "data = pd.read_csv('fivegrams.txt', sep='\\t', names=['freq', '1', '2', '3', '4', '5'])\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "91884bfb0f1142898941dd67ab1afdab",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1044268 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "n_grams = [Counter() for _ in range(N + 1)]\n",
        "\n",
        "# saving all 1-5 grams\n",
        "\n",
        "for _, row in tqdm(data.iterrows(), total=len(data)):\n",
        "    five_gram = (row['1'], row['2'], row['3'], row['4'], row['5'])\n",
        "\n",
        "    for n in range(N + 1):\n",
        "        for j in range(N - n + 1):\n",
        "            n_grams[n][five_gram[j:j+n]] += row['freq']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(('at', 'the', 'end', 'of', 'the'), 13588),\n",
              " (('i', 'do', \"n't\", 'want', 'to'), 12744),\n",
              " (('in', 'the', 'middle', 'of', 'the'), 9124),\n",
              " (('i', 'do', \"n't\", 'know', 'what'), 8076),\n",
              " (('you', 'do', \"n't\", 'have', 'to'), 7186),\n",
              " (('i', 'do', \"n't\", 'know', 'if'), 6455),\n",
              " (('for', 'the', 'first', 'time', 'in'), 6006),\n",
              " (('i', 'do', \"n't\", 'think', 'it'), 5559),\n",
              " (('there', 'are', 'a', 'lot', 'of'), 5523),\n",
              " (('i', 'do', \"n't\", 'think', 'that'), 5466)]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "n_grams[5].most_common(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "defaultdict(set,\n",
              "            {'q': {'a', 's', 'w'},\n",
              "             'w': {'a', 'd', 'e', 'q', 's'},\n",
              "             'e': {'d', 'f', 'r', 's', 'w'},\n",
              "             'r': {'d', 'e', 'f', 'g', 't'},\n",
              "             't': {'f', 'g', 'h', 'r', 'y'},\n",
              "             'y': {'g', 'h', 'j', 't', 'u'},\n",
              "             'u': {'h', 'i', 'j', 'k', 'y'},\n",
              "             'i': {'j', 'k', 'l', 'o', 'u'},\n",
              "             'o': {'i', 'k', 'l', 'p'},\n",
              "             'p': {'l', 'o'},\n",
              "             'a': {'q', 's', 'w', 'x', 'z'},\n",
              "             's': {'a', 'c', 'd', 'e', 'q', 'w', 'x', 'z'},\n",
              "             'd': {'c', 'e', 'f', 'r', 's', 'v', 'w', 'x'},\n",
              "             'f': {'b', 'c', 'd', 'e', 'g', 'r', 't', 'v'},\n",
              "             'g': {'b', 'f', 'h', 'n', 'r', 't', 'v', 'y'},\n",
              "             'h': {'b', 'g', 'j', 'm', 'n', 't', 'u', 'y'},\n",
              "             'j': {'h', 'i', 'k', 'm', 'n', 'u', 'y'},\n",
              "             'k': {'i', 'j', 'l', 'm', 'o', 'u'},\n",
              "             'l': {'i', 'k', 'o', 'p'},\n",
              "             'z': {'a', 's', 'x'},\n",
              "             'x': {'a', 'c', 'd', 's', 'z'},\n",
              "             'c': {'d', 'f', 's', 'v', 'x'},\n",
              "             'v': {'b', 'c', 'd', 'f', 'g'},\n",
              "             'b': {'f', 'g', 'h', 'n', 'v'},\n",
              "             'n': {'b', 'g', 'h', 'j', 'm'},\n",
              "             'm': {'h', 'j', 'k', 'n'}})"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# getting all neigbouring keys for each key\n",
        "\n",
        "keyboard_rows = ['qwertyuiop', 'asdfghjkl', 'zxcvbnm']\n",
        "\n",
        "keyboard_neighbours = defaultdict(set)\n",
        "\n",
        "for row_idx, row in enumerate(keyboard_rows):\n",
        "    for col_idx, key in enumerate(row):\n",
        "        neighbours = set()\n",
        "        for i in range(max(0, row_idx - 1), min(len(keyboard_rows), row_idx + 2)):\n",
        "            for j in range(max(0, col_idx - 1), min(len(keyboard_rows[i]), col_idx + 2)):\n",
        "                if keyboard_rows[i][j] == key:\n",
        "                    continue\n",
        "                neighbours.add(keyboard_rows[i][j])\n",
        "        keyboard_neighbours[key] = neighbours\n",
        "\n",
        "keyboard_neighbours"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def edit_distance_1(word, weights):\n",
        "    ''' Generate all possible combinations of edit distance 1 for the given word and with given weights '''\n",
        "    \n",
        "    letters = \"abcdefghijklmnopqrstuvwxyz'\"\n",
        "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
        "    deletes = [(L + R[1:], weights[0]) for L, R in splits if R]\n",
        "    transposes = [(L + R[1] + R[0] + R[2:], weights[0]) for L, R in splits if len(R) > 1]\n",
        "    replaces = [(L + c + R[1:], weights[1] if c in keyboard_neighbours[R[0]] else weights[0]) for L, R in splits if R for c in letters]\n",
        "    inserts = [(L + c + R, weights[1] if (L and c in keyboard_neighbours[L[-1]]) or (R and c in keyboard_neighbours[R[0]]) else weights[0]) for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts + [(word, weights[2])])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def interpolated_prob(word, context, lambdas):\n",
        "    ''' Calculate linear interpolation for 1-5 grams of context and word with given lambdas '''\n",
        "\n",
        "    total_prob = 0\n",
        "    \n",
        "    for n in range(min(N, len(context) + 1)):\n",
        "        cur_context = context[-n:] if n > 0 else ()\n",
        "        context_freq = n_grams[n][cur_context]\n",
        "        prob = n_grams[n+1][(*cur_context, word)] / context_freq if context_freq > 0 else 0\n",
        "        total_prob += lambdas[n] * prob\n",
        "    \n",
        "    return total_prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def word_candidates(word, context, weights, lambdas):\n",
        "    ''' For given word, context and hyperparameters get candidate words with their unnormalized probabilities '''\n",
        "\n",
        "    candidates = {}\n",
        "    for x in edit_distance_1(word, weights):\n",
        "        if (x[0],) in n_grams[1].keys():\n",
        "            if x[0] in candidates.keys():\n",
        "                candidates[x[0]] = max(candidates[x[0]], x[1])\n",
        "            else:\n",
        "                candidates[x[0]] = x[1]\n",
        "    return [(x[0], x[1] * interpolated_prob(x[0], context, lambdas)) for x in candidates.items()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def beam_search(tokens, weights, lambdas, beam_width=5):\n",
        "    ''' Beam search implementation for given hyperparameters '''\n",
        "\n",
        "    sequences = [[[], 1.0]]\n",
        "\n",
        "    for i in range(len(tokens)):\n",
        "        if tokens[i] in punct:\n",
        "            continue\n",
        "\n",
        "        next_sequences = []\n",
        "        for seq, score in sequences:\n",
        "            context = tuple(seq[-(N - 1):])\n",
        "            next_word_candidates = word_candidates(tokens[i], context, weights, lambdas)\n",
        "            if len(next_word_candidates) == 0:\n",
        "                next_word_candidates = [(tokens[i], 1)]\n",
        "            for next_word, next_word_prob in next_word_candidates:\n",
        "                new_seq = seq + [next_word]\n",
        "                new_score = score * next_word_prob\n",
        "                next_sequences.append([new_seq, new_score])\n",
        "        next_sequences.sort(key=lambda x: x[1], reverse=True)\n",
        "        sequences = next_sequences[:beam_width]\n",
        "    return sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "twd = TreebankWordDetokenizer()\n",
        "\n",
        "def correct_spelling(text, weights, lambdas):\n",
        "    ''' Correct spelling in the given text '''\n",
        "    \n",
        "    tokens = word_tokenize(text.lower())\n",
        "    best_result = beam_search(tokens, weights, lambdas)[0][0]\n",
        "    return twd.detokenize(best_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'the quick brown fox jump ocwr the lazy dog'"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# example of correction\n",
        "correct_spelling(\"Teh quirk brawn fix jumsp ocwr hte laizy doge\", [1, 1.5, 2], [0.1, 0.15, 0.2, 0.25, 0.3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "OwZWaX9VVs7B"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['The quick brown fox jumps over the lazy dog.',\n",
              " 'My Mum tries to be cool by saying that she likes all the same things that I do.',\n",
              " 'A purple pig and a green donkey flew a kite in the middle of the night and ended up sunburnt.',\n",
              " 'Last Friday I saw a spotted striped blue worm shake hands with a legless lizard.',\n",
              " \"A song can make or ruin a person's day if they let it get to them.\"]"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# open data for dev set and test set \n",
        "test_data = open('test_sentences.txt', 'r', encoding='utf-8').read().replace('â€™', '\\'').replace('â€¦', '').split('\\n')\n",
        "test_data[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# split into dev set and test set\n",
        "random.seed(1337)\n",
        "dev_set_len = int(0.2 * len(test_data))\n",
        "random.shuffle(test_data)\n",
        "dev_set = test_data[:dev_set_len]\n",
        "test_set = test_data[dev_set_len:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "cum_prob_swap = 0.15\n",
        "cum_prob_neighbour = cum_prob_swap + 0.35\n",
        "cum_prob_skipped = cum_prob_neighbour + 0.2\n",
        "cum_prob_inserted_neighbour = cum_prob_skipped + 0.2\n",
        "cum_prob_inserted_random = cum_prob_inserted_neighbour + 0.05\n",
        "\n",
        "letters = \"abcdefghijklmnopqrstuvwxyz\"\n",
        "\n",
        "def misspell_word(word, misspell_prob=0.1):\n",
        "    ''' Misspell every letter of the given word with the given probability '''\n",
        "\n",
        "    new_word = ''\n",
        "    skip_next = False\n",
        "    for i, letter in enumerate(word):\n",
        "        if skip_next:\n",
        "            skip_next = False\n",
        "            continue\n",
        "\n",
        "        if letter.isalpha() and random.random() < misspell_prob:\n",
        "            misspell_type_roll = random.random()\n",
        "            if misspell_type_roll < cum_prob_swap and i < len(word) - 1:\n",
        "                new_word += word[i + 1] + word[i]\n",
        "                skip_next = True\n",
        "            elif misspell_type_roll < cum_prob_neighbour:\n",
        "                new_word += random.choice(list(keyboard_neighbours[letter]))\n",
        "            elif misspell_type_roll < cum_prob_skipped:\n",
        "                continue\n",
        "            elif misspell_type_roll < cum_prob_inserted_neighbour:\n",
        "                new_word += letter + random.choice(list(keyboard_neighbours[letter]))\n",
        "            elif misspell_type_roll < cum_prob_inserted_random:\n",
        "                new_word += letter + random.choice(letters)\n",
        "            else:\n",
        "                new_word += random.choice(letters)\n",
        "        else:\n",
        "            new_word += letter\n",
        "    return new_word\n",
        "\n",
        "def misspell_sentence(sent, misspell_prob=0.1):\n",
        "    ''' Misspell the whole sentence '''\n",
        "\n",
        "    tokens = [x for x in word_tokenize(sent.lower()) if x not in punct]\n",
        "    while True:\n",
        "        new_tokens = []\n",
        "        for token in tokens:\n",
        "            new_tokens.append(misspell_word(token, misspell_prob))\n",
        "        if new_tokens != tokens:\n",
        "            break\n",
        "    return twd.detokenize(new_tokens)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_metrics(true_sent, miss_sent, pred_sent):\n",
        "    ''' Calculate accuracy, precision, recall and f1 score for the given correction of the sentence '''\n",
        "\n",
        "    true_tokens = [x for x in word_tokenize(true_sent.lower()) if x not in punct]\n",
        "    miss_tokens = word_tokenize(miss_sent.lower())\n",
        "    pred_tokens = word_tokenize(pred_sent.lower())\n",
        "\n",
        "    tp = sum([1 for true_token, miss_token, pred_token in zip(true_tokens, miss_tokens, pred_tokens) if true_token != miss_token and true_token == pred_token])\n",
        "    fn = sum([1 for true_token, miss_token, pred_token in zip(true_tokens, miss_tokens, pred_tokens) if true_token != miss_token and true_token != pred_token])\n",
        "    fp = sum([1 for true_token, miss_token, pred_token in zip(true_tokens, miss_tokens, pred_tokens) if true_token == miss_token and true_token != pred_token])\n",
        "\n",
        "    accuracy = sum([1 for true_token, pred_token in zip(true_tokens, pred_tokens) if true_token == pred_token]) / len(true_tokens)\n",
        "    precision = tp / (tp + fp) if tp > 0 else 0\n",
        "    recall = tp / (tp + fn) if tp > 0 else 0\n",
        "    f1_score = tp / (tp + 0.5 * (fp + fn))\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1_score,\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Genetic Algorithm for parameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "w_range = np.arange(1, 2, 0.05)\n",
        "lambda_range = np.arange(0.1, 1, 0.05)\n",
        "\n",
        "def generate_gene():\n",
        "    ''' Generate random gene of hyperparameters in form [default_weight, neigbouring_key_weight, self_weight (don't change the word), 5x lambdas] '''\n",
        "\n",
        "    ws =  [round(random.choice(w_range), 2) for _ in range(3)]\n",
        "    ls = [round(random.choice(lambda_range), 2) for _ in range(5)]\n",
        "    return ws + ls\n",
        "\n",
        "def mutate(gene):\n",
        "    ''' Slightly modify given gene '''\n",
        "\n",
        "    new_gene = []\n",
        "    for  x in gene:\n",
        "        prob = random.random()\n",
        "        mult = 0\n",
        "        if prob < 0.2:\n",
        "            mult = -1\n",
        "        elif prob > 0.8:\n",
        "            mult = 1\n",
        "        new_gene.append(round(x + mult * 0.05, 2))\n",
        "    return new_gene\n",
        "\n",
        "def merge(gene1, gene2):\n",
        "    ''' Cross 2 given genes and return created children '''\n",
        "    \n",
        "    x = random.randint(1, len(gene1) - 1)\n",
        "    child1 = gene1[:x] + gene2[x:]\n",
        "    child2 = gene2[:x] + gene1[x:]\n",
        "    return [child1, child2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def objective_function(gene):\n",
        "    ''' Function to be optimized by genetic algorithm (f1 score) '''\n",
        "    \n",
        "    cur_f1 = 0\n",
        "    random.seed(1337)\n",
        "    for test_sent in dev_set:\n",
        "        misspelled = misspell_sentence(test_sent)\n",
        "        corrected = correct_spelling(misspelled, gene[:3], gene[3:])\n",
        "        cur_f1 += calculate_metrics(test_sent, misspelled, corrected)['f1']\n",
        "\n",
        "    return cur_f1 / len(dev_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "NUM_ITERATIONS = 20\n",
        "POPULATION_SIZE = 16\n",
        "BEST_SIZE = 8\n",
        "\n",
        "# initial population\n",
        "population = sorted([generate_gene() for _ in range(POPULATION_SIZE)], key=objective_function, reverse=True)\n",
        "\n",
        "progress_bar = tqdm(range(NUM_ITERATIONS))\n",
        "\n",
        "for i in progress_bar:\n",
        "    # each iteration take best genes from the population and merge and mutate them, also add some new random genes for divesity\n",
        "    for j in range(0, BEST_SIZE, 2):\n",
        "        child1, child2 = merge(population[j], population[j + 1])\n",
        "        population.append(mutate(child1))\n",
        "        population.append(mutate(child2))\n",
        "        population.append(generate_gene())\n",
        "\n",
        "    population = sorted(population, key=objective_function, reverse=True)[:POPULATION_SIZE]\n",
        "\n",
        "    progress_bar.set_description(f'Best F1: {objective_function(population[0])}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_weights = population[0][:3]\n",
        "best_lambdas = population[0][3:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# saved best gene, so I don't have to run genetic algorithm every time\n",
        "best_gene = [0.05, 0.1, 5.5, 0.2, 0.35, 0.4, 0.25, 0.6]\n",
        "best_weights = best_gene[:3]\n",
        "best_lambdas = best_gene[3:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oML-5sJwGRLE"
      },
      "source": [
        "## Justify your decisions\n",
        "\n",
        "Write down justificaitons for your implementation choices. For example, these choices could be:\n",
        "- Which ngram dataset to use\n",
        "- Which weights to assign for edit1, edit2 or absent words probabilities\n",
        "- Beam search parameters\n",
        "- etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Xb_twOmVsC6"
      },
      "source": [
        "- For my Language Model I used [five-grams dataset](https://www.ngrams.info/download_coca.asp), however not only five-grams were used. \n",
        "- I used linear interpolation, so summing probabilities of unigram, bigram, ..., five-gram each weighted by some hyperparameter Î». This is useful, because sometimes less context is better.\n",
        "- As for edit distances, only edit distance of 1 was considered, because using edit distance 2 requires much more time to process.\n",
        "- For edit distance 1 keyboard layout was taken into account. So for example word \"tewt\" is more likly to be \"test\" than \"text\", because the letter \"s\" is close to the \"w\".\n",
        "- If the original unedited word is not in the vocabulary, a weight is assigned to it. For example, if we get a word \"google\" it should not be corrected, even though it is not present in the vocabulary. \n",
        "- For sampling method beam search was used, with beam width of 5, beacuse five-grams were used.\n",
        "- All hyperparameters (weights for each edit and lambdas for liner interpolation) were tuned on dev set using genetic algorithm, using f1 score as an objective function.\n",
        "- [This dataset](https://www.kaggle.com/datasets/nikitricky/random-english-sentences/data) was misspelled and used for dev set and test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46rk65S4GRSe"
      },
      "source": [
        "## Evaluate on a test set\n",
        "\n",
        "Your task is to generate a test set and evaluate your work. You may vary the noise probability to generate different datasets with varying compexity. Compare your solution to the Norvig's corrector, and report the accuracies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "# copy pasted Norvig's solution for performance comparison\n",
        "\n",
        "import re\n",
        "\n",
        "def words(text): return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "WORDS = Counter(words(open('big.txt').read()))\n",
        "\n",
        "def P(word, N=sum(WORDS.values())): \n",
        "    \"Probability of `word`.\"\n",
        "    return WORDS[word] / N\n",
        "\n",
        "def norvig_correction(word): \n",
        "    \"Most probable spelling correction for word.\"\n",
        "    return max(candidates(word), key=P)\n",
        "\n",
        "def candidates(word): \n",
        "    \"Generate possible spelling corrections for word.\"\n",
        "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
        "\n",
        "def known(words): \n",
        "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
        "    return set(w for w in words if w in WORDS)\n",
        "\n",
        "def edits1(word):\n",
        "    \"All edits that are one edit away from `word`.\"\n",
        "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "def edits2(word): \n",
        "    \"All edits that are two edits away from `word`.\"\n",
        "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
        "\n",
        "def norvig_correct_sentence(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    corrected_tokens = [norvig_correction(x) for x in tokens]\n",
        "    return twd.detokenize(corrected_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b177b1b04e294eb1a05b5605361395c3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/580 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Misspelling Probability: 0.05\n",
            "My metrics:       ['accuracy: 0.88609', 'precision: 0.74256', 'recall: 0.73940', 'f1: 0.71013']\n",
            "Norvig's metrics: ['accuracy: 0.87026', 'precision: 0.70586', 'recall: 0.63633', 'f1: 0.63557']\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6e933e8ccf07488fae18d30e4da9b500",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/580 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Misspelling Probability: 0.1\n",
            "My metrics:       ['accuracy: 0.82201', 'precision: 0.80125', 'recall: 0.64749', 'f1: 0.68672']\n",
            "Norvig's metrics: ['accuracy: 0.81258', 'precision: 0.79899', 'recall: 0.59108', 'f1: 0.64914']\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c45f0a5e28294867b91a62afda673750",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/580 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Misspelling Probability: 0.2\n",
            "My metrics:       ['accuracy: 0.67882', 'precision: 0.88169', 'recall: 0.51326', 'f1: 0.62401']\n",
            "Norvig's metrics: ['accuracy: 0.67784', 'precision: 0.89506', 'recall: 0.49940', 'f1: 0.61349']\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "007275316c204bd9be2d08f9e5b4778b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/580 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Misspelling Probability: 0.5\n",
            "My metrics:       ['accuracy: 0.30551', 'precision: 0.86347', 'recall: 0.23515', 'f1: 0.35514']\n",
            "Norvig's metrics: ['accuracy: 0.31745', 'precision: 0.86894', 'recall: 0.24615', 'f1: 0.36802']\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "264182d46c4546cb8c670f3268b4430c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/580 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Misspelling Probability: 0.9\n",
            "My metrics:       ['accuracy: 0.06589', 'precision: 0.48534', 'recall: 0.05993', 'f1: 0.10443']\n",
            "Norvig's metrics: ['accuracy: 0.06111', 'precision: 0.47198', 'recall: 0.05396', 'f1: 0.09478']\n"
          ]
        }
      ],
      "source": [
        "# testing language model performance on text with different misspell density\n",
        "misspell_probabilities = [0.05, 0.1, 0.2, 0.5, 0.9]\n",
        "\n",
        "for misspell_prob in misspell_probabilities:\n",
        "    my_metrics = {'accuracy': 0.0, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
        "    norvig_metrics = {'accuracy': 0.0, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
        "\n",
        "    random.seed(1337)\n",
        "\n",
        "    for test_sent in tqdm(test_set):\n",
        "        misspelled = misspell_sentence(test_sent, misspell_prob=misspell_prob)\n",
        "\n",
        "        corrected = correct_spelling(misspelled, best_weights, best_lambdas)\n",
        "        my_metrics = {k: my_metrics[k] + v for k, v in calculate_metrics(test_sent, misspelled, corrected).items()}\n",
        "\n",
        "        norvig_corrected = norvig_correct_sentence(misspelled)\n",
        "        norvig_metrics = {k: norvig_metrics[k] + v for k, v in calculate_metrics(test_sent, misspelled, norvig_corrected).items()}\n",
        "\n",
        "    my_metrics = {k: v / len(test_set) for k, v in my_metrics.items()}\n",
        "    norvig_metrics = {k: v / len(test_set) for k, v in norvig_metrics.items()}\n",
        "\n",
        "    print(f'Misspelling Probability: {misspell_prob}\\nMy metrics:       {[f\"{k}: {v:.5f}\" for k, v in my_metrics.items()]}\\nNorvig\\'s metrics: {[f\"{k}: {v:.5f}\" for k, v in norvig_metrics.items()]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It can be seen that my solution performs nearly identical to Norvig's solution, even though I only use words of edit distance 1. I believe that means with more computational power and edit distance of 2, my solution would perform much better than simplistic solution of Norvig."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
